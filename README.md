Tech Stack I Used
Frontend
Next.js (React) for the UI and mic control

Web APIs:

Web Audio API â€“ for real-time audio manipulation

SpeechRecognition â€“ to understand what you say

SpeechSynthesis â€“ to talk back to you

Backend
FastAPI (Python) to process audio intent text and return smart responses

(No paid APIs used â€” itâ€™s all open source and browser-powered!)







 
What the Experience Feels Like


Click the Start Assistant button.

It says: â€œHi Kevin, Iâ€™m listening. Say something like add reverb or stop.â€

You say: â€œAdd delayâ€ â†’ and boom! ğŸ‰ The delay effect is added to your voice.

Assistant: â€œAdding delay. What would you like to do next?â€

You say: â€œPitch shift upâ€ â†’ done.

You say: â€œStopâ€ â†’ it shuts everything down smooth


ğŸ—‚ï¸ Project Structure

soundverse-assistant/
â”œâ”€â”€ backend-fastapi/
â”‚   â””â”€â”€ main.py        # FastAPI logic
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ pages/
â”‚       â””â”€â”€ index.js   # Main assistant UI

How to run

Frontend (Next.js)

cd soundverse-assistant
npm install
npm run dev

Backend (FastAPI)

cd backend-fastapi
python -m venv venv
venv\Scripts\activate  # Windows
pip install fastapi uvicorn
uvicorn main:app --reload --port 8000
